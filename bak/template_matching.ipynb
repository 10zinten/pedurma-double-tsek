{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp double_tsek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "from MTM import matchTemplates, drawBoxesOnRGB\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TengyurConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "def plot(img, cmap=None, sz=(10, 10), axis=False):\n",
    "    plt.figure(figsize=sz)\n",
    "    plt.grid(True)\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "        plt.grid(False)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def to_box(coord):\n",
    "#     x, y, w, h = coord\n",
    "#     x2, y2 = x+w, y+h\n",
    "#     return x, y, x2, y2\n",
    "\n",
    "\n",
    "# def create_template(img_path, coord, t_fn=None, template=False):\n",
    "#     img = cv2.imread(str(img_path))\n",
    "#     print(img.shape)\n",
    "#     if not template:\n",
    "#         img = cv2.resize(img, (config.img_size[1], config.img_size[0]))\n",
    "#     img_copy = img.copy()\n",
    "#     x1, y1, x2, y2 = to_box(coord)\n",
    "#     cv2.rectangle(img_copy, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "    \n",
    "#     x, y, w, h = coord\n",
    "#     template = img[y:y+h, x:x+w]\n",
    "#     plot(template)\n",
    "#     plot(img_copy, sz=(25, 25))\n",
    "\n",
    "#     if t_fn:\n",
    "#         cv2.imwrite(str(t_fn), template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# coord = (2139, 1282, 18, 135) # (x, y, w, h)\n",
    "# create_template('data/test-mantra.jpg', coord, t_fn=config.template_path/'double_tsek_02.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# img_path = config.images_path/'I1PD95846'/'I1PD958460141.jpg'\n",
    "# coord = (1162, 1910, 13, 135) # (x, y, w, h)\n",
    "# create_template(img_path, coord, t_fn=config.template_path/'double_tsek_03.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pure OpenCV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### detect paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# #image = cv2.imread('data/test.jpeg')\n",
    "# image = cv2.imread('data/test_diff_size.jpeg')\n",
    "# image = imutils.resize(image, height=3969, width=2645)\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# blur = cv2.GaussianBlur(gray, (7,7), 0)\n",
    "# thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# # Create rectangular structuring element and dilate\n",
    "# kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "# dilate = cv2.dilate(thresh, kernel, iterations=7)\n",
    "# plot(dilate, cmap='gray', sz=(25, 25))\n",
    "\n",
    "# # Find contours and draw rectangle\n",
    "# cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "# for c in cnts:\n",
    "#     x,y,w,h = cv2.boundingRect(c)\n",
    "#     cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n",
    "\n",
    "# plot(image, sz=(25, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Text Skew Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from deskew import determine_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def image_deskew2(image, show_diff=False):\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     gray = cv2.bitwise_not(gray)\n",
    "#     thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "#     coords = np.column_stack(np.where(thresh > 2))\n",
    "#     angle = cv2.minAreaRect(coords)[-1]\n",
    "\n",
    "#     if angle < -45:\n",
    "#         angle = -(90 + angle)\n",
    "#     else:\n",
    "#         angle = -angle\n",
    "\n",
    "#     # rotate the image to deskew it\n",
    "#     (h, w) = image.shape[:2]\n",
    "#     center = (w // 2, h // 2)\n",
    "#     M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "#     rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "#     print(f'[INFO] Image dskewed by {angle:.3} angles')\n",
    "    \n",
    "#     if show_diff:\n",
    "#         plot(image, sz=(15, 15), axis=True)\n",
    "#         plot(rotated, sz=(15, 15), axis=True)\n",
    "        \n",
    "#     return rotated\n",
    "\n",
    "# def image_deskew(image, show_diff=False):\n",
    "#     def rotate(image, angle, background):\n",
    "#         old_width, old_height = image.shape[:2]\n",
    "#         angle_radian = math.radians(angle)\n",
    "#         width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)\n",
    "#         height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)\n",
    "\n",
    "#         image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "#         rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "#         rot_mat[1, 2] += (width - old_width) / 2\n",
    "#         rot_mat[0, 2] += (height - old_height) / 2\n",
    "#         return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)\n",
    "    \n",
    "#     if isinstance(image, (str, Path)):\n",
    "#         image = cv2.imread(str(image))\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     angle = determine_skew(gray)\n",
    "#     backgroud = tuple([int(x) for x in image[10][10]])\n",
    "#     rotated = rotate(image, angle, backgroud)\n",
    "    \n",
    "#     print(f'[INFO] Image dskewed by {angle:.4} angles')\n",
    "    \n",
    "#     if show_diff:\n",
    "#         plot(image, sz=(15, 15), axis=True)\n",
    "#         plot(rotated, sz=(15, 15), axis=True)\n",
    "        \n",
    "#     return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# _ = image_deskew(cv2.imread('data/peydurma/test-set/white_skewed_01.jpg'), show_diff=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# _ = image_deskew(cv2.imread('data/peydurma/test-set/white_skewed_dtsek_01.jpg'), show_diff=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Template matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def sorted_matches(matches):\n",
    "#     h_sorted_match = []\n",
    "#     for x, y in matches:\n",
    "#         found_group = False\n",
    "#         if h_sorted_match:\n",
    "#             for h_list in h_sorted_match:\n",
    "#                 if abs(y-h_list[0][1]) < 5:\n",
    "#                     h_list.append((x, y))\n",
    "#                     found_group = True\n",
    "#         else:\n",
    "#             h_sorted_match.append([(x, y)])\n",
    "#             found_group = True\n",
    "\n",
    "#         if not found_group:\n",
    "#             h_sorted_match.append([(x, y)])\n",
    "        \n",
    "#     full_sorted_match = []\n",
    "#     for h_list in h_sorted_match:\n",
    "#         full_sorted_match.append(sorted(h_list, key=lambda x: x[0]))\n",
    "        \n",
    "#     return sum(full_sorted_match, [])\n",
    "    \n",
    "\n",
    "# def remove_dup_match(match_locations):\n",
    "#     cleaned_match = []\n",
    "#     prev_x, prev_y = 0, 0\n",
    "#     th = 2\n",
    "#     for x, y in sorted_matches(zip(match_locations[1], match_locations[0])):\n",
    "#         if abs(x-prev_x) < 5 and abs(y-prev_y) < 5: continue\n",
    "#         cleaned_match.append((x, y))\n",
    "#         prev_x, prev_y = x, y\n",
    "#     return cleaned_match\n",
    "\n",
    "\n",
    "# def template_match(img, templates):\n",
    "#     # create edged image\n",
    "#     if isinstance(img, str):\n",
    "#         img = cv2.imread(img)\n",
    "#     if size:\n",
    "#         img = imutils.resize(img, height=config.img_size[0], width=config.img_size[1])\n",
    "#     print('Image size:', img.shape)\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     #plot(gray, cmap='gray', sz=(50, 50))\n",
    "#     edged = cv2.Canny(gray, 100, 600)\n",
    "#     #plot(edged, cmap='gray', sz=(50, 50))\n",
    "    \n",
    "#     output = defaultdict(list)\n",
    "#     clone = img.copy()\n",
    "#     for template_ in templates:\n",
    "#         t_type, template, th, data = template_\n",
    "        \n",
    "#         # template matching\n",
    "#         result = cv2.matchTemplate(edged, template, cv2.TM_CCOEFF)\n",
    "#         min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "#         max_thresh = max_val * th\n",
    "#         match_locations = np.where(result>=max_thresh)\n",
    "#         cleaned_match_locations = remove_dup_match(match_locations)\n",
    "        \n",
    "#         # Plot\n",
    "#         w, h = template.shape[::-1]\n",
    "#         for (x, y) in cleaned_match_locations:\n",
    "#             output[t_type].append((x, y))\n",
    "#             cv2.rectangle(clone, (x, y), (x+w, y+h), [0,0,255], 2)\n",
    "    \n",
    "#         print(f'No. {t_type} detected: {len(output[t_type])}')\n",
    "\n",
    "#     plot(clone, cmap='gray', sz=(25, 25))\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# templates = [\n",
    "#     ('rectangle', rect_template, 0.9, {}),\n",
    "#     #('circle', cir_template, 0.7, {'radius': radius})\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#output = template_match('data/test-mantra.jpg', templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Template-Matching Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_templates(path):\n",
    "    templates = []\n",
    "    for p in Path(path).iterdir():\n",
    "        if not p.name.endswith('.png'): continue\n",
    "        templates.append((p.stem, cv2.imread(str(p))))\n",
    "    return templates\n",
    "\n",
    "def mtm(image, templates, show=False, th=0.9):\n",
    "    if isinstance(image, (str, Path)):\n",
    "        image = cv2.imread(str(image))\n",
    "    matches = []\n",
    "    try:\n",
    "        hits = matchTemplates(templates, image, score_threshold=th, method=cv2.TM_CCOEFF_NORMED, maxOverlap=0.3)\n",
    "        for x, y, w, h in list(hits['BBox']):\n",
    "            matches.append([x, y, x+w, y+h])\n",
    "        if show: image = drawBoxesOnRGB(image, hits, boxThickness=5, boxColor=(255,0,0))\n",
    "    except KeyError as ex:\n",
    "        if ex.args[0] == 'Score':\n",
    "            print('\\t- double tsek not found !')\n",
    "            return matches\n",
    "\n",
    "    print(f'\\t- no. of double tsek detected: {len(matches)}')\n",
    "    if show:\n",
    "        plot(image, sz=(15, 15))\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "templates = get_templates(config.template_path); len(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_output = mtm('data/test.jpeg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtm('data/test-mantra.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtm('data/test-02.jpeg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtm('data/test_diff_size.jpeg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mtm('data/test-03.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test on actual Peydurma Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#mtm('data/peydurma-05.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#mtm('data/peydurma/test-set/yellow_01.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#mtm('data/peydurma/test-set/yellow_02.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#skewed_output = mtm('data/peydurma/test-set/white_skewed_dtsek_01.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# deskewed_img = image_deskew(config.images_path/'I1PD95846'/'I1PD958460141.jpg')\n",
    "# plot(deskewed_img)\n",
    "#mtm(config.images_path/'I1PD95846'/'I1PD958460141.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#mtm('data/peydurma/test-set/non_breaking.jpg', templates, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find reinsertion span\n",
    "find line number and char location of double tsek\n",
    "- input: ocr_boxes, match_loc\n",
    "- output: line number and char index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get OCR output\n",
    "- unzip ocr output and read the response json\n",
    "- resize the box w.r.t config.img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ocr_output(path):\n",
    "    imagegroup, img_fn = path.parts[-2:]\n",
    "    res_fn = config.ocr_output_path/imagegroup/f'{img_fn.split(\".\")[0]}.json.gz'\n",
    "    return json.load(gzip.open(str(res_fn), 'rb'))\n",
    "\n",
    "def get_symbol(response):\n",
    "    for page in response['fullTextAnnotation']['pages']:\n",
    "        for block in page['blocks']:\n",
    "            for paragraph in block['paragraphs']:\n",
    "                for word in paragraph['words']:\n",
    "                    for symbol in word['symbols']:\n",
    "                        char = symbol['text']\n",
    "                        v = symbol['boundingBox']['vertices']\n",
    "                        box = [v[0]['x'], v[0]['y'], v[2]['x'], v[2]['y']]\n",
    "                        yield char, box\n",
    "\n",
    "def get_full_text_annotations(response):\n",
    "    boxes, text = [], ''\n",
    "    for char, box in get_symbol(response):\n",
    "        text += char\n",
    "        boxes.append(box)\n",
    "    return boxes, text\n",
    "\n",
    "def resize_boxes(boxes, old_size):\n",
    "    \"`boxes` are in top-right and bottom-left coord system.\"\n",
    "    h, w = old_size[:2]\n",
    "    h_scale = config.img_size[0]/h\n",
    "    w_scale = config.img_size[1]/w\n",
    "    result = []\n",
    "    for box in boxes:\n",
    "        # adjust the box\n",
    "        box[0] *= w_scale\n",
    "        box[1] *= h_scale\n",
    "        box[2] *= w_scale\n",
    "        box[3] *= h_scale\n",
    "        box = list(map(int, box))\n",
    "        result.append(box)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes(img, boxes, show=True, color=[0,0,255]):\n",
    "    for x1, y1, x2, y2 in boxes[0]:\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "    if len(boxes) > 1:\n",
    "        for x1, y1, x2, y2 in boxes[1]:\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), [255,0,0], 5)\n",
    "    if show: plot(img, sz=(25, 25))\n",
    "    else: return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_resize_boxes(image_path):\n",
    "    image = cv2.imread(str(image_path))\n",
    "    old_size = image.shape\n",
    "    image = cv2.resize(image, (config.img_size[1], config.img_size[0]))\n",
    "    response = get_ocr_output(image_path)\n",
    "    boxes, text = get_full_text_annotations(response)\n",
    "    print(text)\n",
    "    boxes = resize_boxes(boxes, old_size)\n",
    "    plot_boxes(image, [boxes, []])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_resize_boxes(config.images_path/'I1PD95846'/'I1PD958460142.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_resize_boxes(config.images_path/'I1PD95846'/'I1PD958460141.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get context of Double Tsek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cls_box_into_line(boxes, th=20):\n",
    "    lines = []\n",
    "    line = []\n",
    "    prev_y1 = boxes[0][1]\n",
    "    for box in boxes:\n",
    "        if abs(box[1] - prev_y1) < th:\n",
    "            line.append(box)\n",
    "        else:\n",
    "            lines.append(line)\n",
    "            line = []\n",
    "            line.append(box)\n",
    "        prev_y1 = box[1]\n",
    "    else:\n",
    "        if line: lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_box_into_line(image_path):\n",
    "    image = cv2.imread(str(image_path))\n",
    "    old_size = image.shape\n",
    "    image = cv2.resize(image, (config.img_size[1], config.img_size[0]))\n",
    "    response = get_ocr_output(image_path)\n",
    "    boxes, text = get_full_text_annotations(response)\n",
    "    boxes = resize_boxes(boxes, old_size)\n",
    "    boxe_lines = cls_box_into_line(boxes)\n",
    "    \n",
    "    for box_line in boxe_lines:\n",
    "        r, g, b = map(int, np.random.choice(range(256), size=3))\n",
    "        image = plot_boxes(image, [box_line], show=False, color=(r,g,b))\n",
    "    plot(image, sz=(25, 25))\n",
    "    \n",
    "#test_box_into_line(config.images_path/'I1PD95846'/'I1PD958460048.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_double_tsek_bf(matched_box, boxes, th=20):\n",
    "    box_lines = cls_box_into_line(boxes)\n",
    "    pos = 0\n",
    "    prev_x1 = 0\n",
    "    for box_line in box_lines:\n",
    "        if abs(matched_box[1] - box_line[0][1]) < th:\n",
    "            for i, box in enumerate(box_line):\n",
    "                if matched_box[0] > prev_x1 and matched_box[0] < box[0]:\n",
    "                    pos += i-1\n",
    "                    return pos\n",
    "        pos += len(box_line)\n",
    "\n",
    "\n",
    "def compute_iou(box_arr1, box_arr2):\n",
    "    x11, y11, x12, y12 = np.split(box_arr1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(box_arr2, 4, axis=1)\n",
    "\n",
    "    xA = np.maximum(x11, np.transpose(x21))\n",
    "    yA = np.maximum(y11, np.transpose(y21))\n",
    "    xB = np.minimum(x12, np.transpose(x22))\n",
    "    yB = np.minimum(y12, np.transpose(y22))\n",
    "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
    "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
    "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
    "    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_double_tsek_idx(image_path, templates, deskew=False, show_boxes=False):\n",
    "    # load, deskew and resize the image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    old_size = image.shape\n",
    "    if deskew: image = image_deskew(image)\n",
    "    image = image = cv2.resize(image, (config.img_size[1], config.img_size[0]))\n",
    "\n",
    "    # find the double tsek boxes\n",
    "    matches = mtm(image, templates)\n",
    "\n",
    "    # Get ocr boxes\n",
    "    response = get_ocr_output(image_path)\n",
    "    boxes, text = get_full_text_annotations(response)\n",
    "    if not matches: return [], text\n",
    "    boxes = resize_boxes(boxes, old_size)\n",
    "\n",
    "    # find double tsek char index\n",
    "    iou_matrix = compute_iou(np.array(matches), np.array(boxes))\n",
    "    if show_boxes: plot_boxes(image, [boxes, matches])\n",
    "    idxs = list(np.argmax(iou_matrix, axis=1))\n",
    "    if 0 in idxs:\n",
    "        undetected_box_idx = idxs.index(0)\n",
    "        undetected_box_char_idx = find_double_tsek_bf(matches[undetected_box_idx], boxes)\n",
    "        if undetected_box_char_idx:\n",
    "            idxs[undetected_box_idx] = undetected_box_char_idx\n",
    "        else:\n",
    "            idxs.pop(undetected_box_idx)\n",
    "    idxs.sort()\n",
    "    return idxs, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_get_double_tsek_idx(image_path):\n",
    "    idxs, text = get_double_tsek_idx(image_path, templates, show_boxes=True)\n",
    "    print(idxs)\n",
    "    for cc in idxs:\n",
    "        print(text[cc-10:cc], text[cc], text[cc+1: cc+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_get_double_tsek_idx(config.images_path/'I1PD95846'/'I1PD958460047.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_get_double_tsek_idx(config.images_path/'I1PD95846'/'I1PD958460043.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_get_double_tsek_idx(config.images_path/'I1PD95846'/'I1PD958460048.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def put_whitespaces(text):\n",
    "    result = ''\n",
    "    for chunk in text.split('།'):\n",
    "        if chunk:\n",
    "            result += chunk + '། '\n",
    "        else:\n",
    "            result += '།'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_str = '།ལས་བརྒྱ་ཐམ་པ་པ།༄༅།།རྒྱ་གར་སྐད་དུ།ཀརྨ་ཤ་ཏ་ཀ།བོད་སྐད་དུ།ལས་བརྒྱ་ཐམ་པ་པོ།བམ་པོ་དང་པོ།ཐམས་ཅད་མཁྱེན་པ་ལ་ཕྱག་འཚལ་ལོ།།གང་ལས་འཇིག་རྟེན་བླ་མ་བདེ་གཤེགས་ཐོས་པའི་སྒོ་ནས་རབ་སྙན་བརྟན་པའི་གསུང་་་་ལྡན་གྱིས།།སེམས་ཅན་རྣམས་ལ་ཕན་པ་འབའ་ཞིག་བཞེད་ཕྱིར་བཤད་པ་རྣམ་པ་སྣ་ཚོགས་རང་ཉིད་ཀྱིས།།ལོག་པར་ལྟ་བའི་མུན་ནག་ཆེན་པོ་ཐིབས་པོར་$འཐོམས་ཤིངའཁྲུགས་པ་རྣམས་ལ་རབ་གསུངས་པ།།་$དེ་ཡི་མིང་ནི་ལས་རྣམ་བརྒྱ་པ་ཞེས་བྱ་ཡོངས་སུ་ཚང་བ་བདག་གིས་བཤད་ཀྱིས་ཉོན།།སྤྱི་སྡོམ་ནི༑ཁྱི་མོ་དང་ནི་ཤིང་རྟ་དང་།།ཀ་ཙང་ཀ་ལ་བྱམས་མི་སྡུག།བྱ་དང་འཕྱེ་བོ་གང་པོ་དང་།།བུ་རྣམས་དང་ནི་བརྒྱ་བྱིན་ནོ།།སྡོམ་ནི།ཁྱི་མོ་མིག་ཆུང་ལ་རྫོགས་བྱེད་དང་།།སྒྱུར་གཉིས་འཆར་ཀ་རྒྱལ་མཚན་དང༌།།བདེ་བྱེད་མ་དང་ནོར་བུའི་འོད།།སྣ་མའི་མེ་ཏོག་ང་བྱིན་དང༌།།འདུས་མོ་དང་ནི་ཚེམ་བུ་མདོ་སྡེ།ཧ་མཁན༑།ཁྱི་མོ་ཞེས་བྱ་བ་ནི།གླེང་$གཞི་མཉན་དུ་ཡོད་པ་ན་བཞུགས་ཏེ།དེའི་ཚེ་མཉན་དུ་ཡོད་པ་ནི།ཁྱིམ་བདག་ཕྱུག་ཅིང་ནོར་མང་ལ་ལོངས་སྤྱོད་ཆེ་བ་ཡོངས་ས'\n",
    "# put_whitespaces(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rm_running_head(text):\n",
    "    r_head_end_idx = text.find('༡')\n",
    "    if r_head_end_idx >= 0  and r_head_end_idx < 500:\n",
    "        return text[r_head_end_idx+1:]\n",
    "    else:\n",
    "        return text[text.find('།')+1:]\n",
    "\n",
    "def rm_noise(text):\n",
    "    'remove numbers and etc'\n",
    "    text = re.sub(f'\\d+', '', text)\n",
    "    for r in ['=', '|', '“', '”', ']', '）', ')', '》', '>', '©', '–', '-', '༸', ('་ི', '་')]:\n",
    "        if isinstance(r, tuple):\n",
    "            text = text.replace(r[0], r[1])\n",
    "        else:\n",
    "            text = text.replace(r, '')\n",
    "    return text\n",
    "\n",
    "def postprocess(text):\n",
    "    text = rm_running_head(text)\n",
    "    text = rm_noise(text)\n",
    "    for f, t in [('$་','་$'), ('$།', '།$'), ('།་$', '།$')]:\n",
    "        text = text.replace(f, t)\n",
    "    text = put_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "def str_insert(text, idx, char):\n",
    "    text = text[:idx] + char + text[idx:]\n",
    "    return text\n",
    "\n",
    "def add_double_tsek(text, idxs):\n",
    "    for i, idx in enumerate(idxs):\n",
    "        text = str_insert(text, idx+i, config.double_tsek_sym)\n",
    "    return text\n",
    "\n",
    "def get_double_tsek_text(text_id, path, start, end, engine):\n",
    "    ann_text_fn = config.output_tmp_path/f'{text_id}-{config.pecha_name}-pedurma-ann_text.txt'\n",
    "    if ann_text_fn.is_file():\n",
    "        ann_text = ann_text_fn.read_text()\n",
    "        if engine == 'diff':\n",
    "            return '', ann_text\n",
    "        base_text = ann_text.replace(config.double_tsek_sym, '')\n",
    "        return base_text, ann_text\n",
    "    ann_text = ''\n",
    "    for i, path in enumerate(sorted((path).iterdir()), 1):\n",
    "        if i <  start: continue\n",
    "        if i > end: break\n",
    "        print(f'[INFO] {i+1} - Processing {path.name} ...')\n",
    "        idxs, text = get_double_tsek_idx(path, templates)\n",
    "        ann_text += f'\\n\\n{path.name}' if config.debug else \"\"\n",
    "        ann_text += postprocess(add_double_tsek(text, idxs))\n",
    "        \n",
    "    ann_text_fn.write_text(\n",
    "        ann_text.replace(\n",
    "            config.double_tsek_sym, config.expected_double_tsek_sym\n",
    "        )\n",
    "    )\n",
    "    return base_text, ann_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.debug = True\n",
    "# base_text, ann_text = get_double_tsek_text('T340', config.images_path/'I1PD95846', 0, 0)\n",
    "# print(ann_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_double_tsek_vol_by_pages(path, start, end, engine):\n",
    "    for i, path in enumerate(sorted((path).iterdir()), 1):\n",
    "        if i <  start: continue\n",
    "        if i > end: break\n",
    "        # speed up\n",
    "        ann_page_fn = config.output_tmp_path/f'{path.stem}-ann.txt'\n",
    "        if ann_page_fn.is_file():\n",
    "            ann_page = ann_page_fn.read_text()\n",
    "            if engine == 'diff':\n",
    "                yield '', ann_page, path.stem\n",
    "            else:\n",
    "                base_page = ann_page.replace(config.double_tsek_sym, '')\n",
    "                yield base_page, ann_page, path.stem\n",
    "\n",
    "        print(f'[INFO] {i+1} - Processing {path.name} ...')\n",
    "        idxs, text = get_double_tsek_idx(path, templates)\n",
    "        ann_page = postprocess(add_double_tsek(text, idxs))\n",
    "        ann_page_fn.write_text(ann_page)\n",
    "        if engine == 'diff':\n",
    "            yield '', ann_page, path.stem\n",
    "        else:\n",
    "            base_page = ann_page.replace(config.double_tsek_sym, '')\n",
    "            yield base_page, ann_page, path.stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Annotations\n",
    "\n",
    "Steps:\n",
    "1. Paser peydurm-tengyur text index\n",
    "1. Map peydurma-tengyur text-id to dergey-tengyur text-id\n",
    "1. Extract corresponding dergey-tengyur text\n",
    "1. Extract double-tsek from peyduma-tengyur text\n",
    "1. Create dmp patch of double tsek\n",
    "1. Apply the dmp patch to dergey-tengyur\n",
    "1. Parse the dobule-tsek from dergey-tengyur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parser peydurma tengyur text index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_text_span(node):\n",
    "    vol, span = '', []\n",
    "    for loc in node.getElementsByTagName('outline:location'):\n",
    "        vol = int(loc.attributes['vol'].value)\n",
    "        span.append(int(loc.attributes['page'].value))\n",
    "    if not vol:\n",
    "        return\n",
    "    return {vol: {'start': min(span), 'end': max(span)}}\n",
    "\n",
    "def clean_text_id(text_id):\n",
    "    cats = '  abcd'\n",
    "    text_id = text_id.strip()\n",
    "    if text_id == '00':\n",
    "        return\n",
    "    elif '-' in text_id:\n",
    "        id_, cat = text_id.split('-')\n",
    "        if len(cat) > 1: return\n",
    "        return f'{id_}{cats[int(cat.strip())]}'\n",
    "    elif ';' in text_id:\n",
    "        return text_id.split(';')[0].strip()\n",
    "    else:\n",
    "        return text_id\n",
    "\n",
    "def get_toh(fn):\n",
    "    maps = defaultdict(list)\n",
    "    dom = minidom.parse(str(fn))\n",
    "    for node in dom.getElementsByTagName('outline:node'):\n",
    "        if node.attributes['type'].value != \"text\": continue\n",
    "        for desc_node in node.getElementsByTagName('outline:description'):\n",
    "            attrs = desc_node.attributes\n",
    "            if 'type' in attrs and attrs['type'].value == \"toh\":\n",
    "                desc_childnodes = desc_node.childNodes\n",
    "                if not desc_childnodes: break\n",
    "                text_id = clean_text_id(desc_childnodes[0].data)\n",
    "                if not text_id: break\n",
    "                maps[text_id].append(get_text_span(node))\n",
    "                break\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# toh = get_toh(config.peydurma_meta_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Map peydurma toh to dergey text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def map_text_id(text_id, is_kangyur=False):\n",
    "    if is_kangyur:\n",
    "        return f'T{text_id}'\n",
    "    else:\n",
    "        return f'D{text_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract corresponding dergey-tengyur text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_index_layer(path):\n",
    "    index_layer = yaml.safe_load((path/'index.yml').open())\n",
    "    return index_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_layer = get_index_layer(config.tengyur_opf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_patterns = {\n",
    "    'peydurma_page': { \n",
    "        'start': '<p',\n",
    "        'end': '>',\n",
    "        'has_text': False,\n",
    "        'has_payload': True,\n",
    "        'payload_pattern': '(.*?)'\n",
    "    },\n",
    "    'img_url' : {\n",
    "        'start': '\\[https',\n",
    "        'end': '\\]',\n",
    "        'has_text': False,\n",
    "        'has_payload': True,\n",
    "        'payload_pattern': '(.*?)'\n",
    "    },\n",
    "    'correction': {\n",
    "        'start': '\\(',\n",
    "        'end': '\\)',\n",
    "        'has_text': True,\n",
    "        'has_payload': True,\n",
    "        'payload_pattern': '.*?,(.*?)',\n",
    "        'payload_sep_len': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "def sort_by_pos(matches):\n",
    "    \"\"\"Sort `matches` by start char position\"\"\"\n",
    "    return sorted(matches, key=lambda x: x[1].span(0)[0])\n",
    "\n",
    "\n",
    "def get_base_idx(ann, offset, is_end=False, payload_len=0):\n",
    "    start, end = ann.span(0)\n",
    "    marker_len = end - start\n",
    "    if is_end:\n",
    "        offset += marker_len + payload_len\n",
    "        base_idx = end - offset\n",
    "        return base_idx, offset\n",
    "\n",
    "    base_idx = start - offset\n",
    "    offset += marker_len\n",
    "    return base_idx, offset\n",
    "\n",
    "\n",
    "def get_markers(name, pattern, text):\n",
    "    return [(name, m) for m in re.finditer(pattern, pg_text)]\n",
    "\n",
    "\n",
    "def get_payload(name, start_match, end_match, text):\n",
    "    start = start_match.span(0)[0]\n",
    "    end = end_match.span(0)[1]\n",
    "    search_text = text[start:end]\n",
    "    ann = ann_patterns[name]\n",
    "    payload_pattern = f'{ann[\"start\"]}{ann[\"payload_pattern\"]}{ann[\"end\"]}'\n",
    "    payload = re.findall(payload_pattern, search_text)[0]\n",
    "    return payload\n",
    "\n",
    "\n",
    "def parse_annotations(text, ann_patterns):\n",
    "    # Find start and end of annotation separately\n",
    "    # TODO: apply multiprocessing on patterns\n",
    "    ann_start_markers, ann_ends_markers = [], []\n",
    "    for name in ann_patterns:\n",
    "        ann_start_markers.extend(\n",
    "            get_markers(name, ann_patterns[name][\"start\"], pg_text)\n",
    "        )\n",
    "        ann_ends_markers.extend(get_markers(name, ann_patterns[name][\"end\"], pg_text))\n",
    "\n",
    "    # Sort all_starts and all_ends sperately\n",
    "    ann_start_markers = sort_by_pos(ann_start_markers)\n",
    "    ann_ends_markers = sort_by_pos(ann_ends_markers)\n",
    "\n",
    "    # Find ann_start and ann_end pair\n",
    "    anns = defaultdict(list)\n",
    "    s_idx, e_idx = 0, 0\n",
    "    offset = 0\n",
    "    while s_idx < len(ann_start_markers) or e_idx < len(ann_ends_markers):\n",
    "        #         import pdb; pdb.set_trace()\n",
    "        payload = 0\n",
    "        ann_name, ann_start_match = ann_start_markers[s_idx]\n",
    "        _, ann_end_match = ann_ends_markers[e_idx]\n",
    "        base_start, offset = get_base_idx(ann_start_match, offset)\n",
    "\n",
    "        # ann which includes text\n",
    "        if ann_patterns[ann_name][\"has_text\"]:\n",
    "            # ann with payload\n",
    "            if ann_patterns[ann_name][\"has_payload\"]:\n",
    "                payload = get_payload(ann_name, ann_start_match, ann_end_match, text)\n",
    "                payload_len = len(payload) + ann_patterns[ann_name][\"payload_sep_len\"]\n",
    "            base_end, offset = get_base_idx(\n",
    "                ann_end_match, offset, is_end=True, payload_len=payload_len\n",
    "            )\n",
    "\n",
    "            anns[ann_name].append((base_start, base_end, payload))\n",
    "        # ann which doesn't includes text\n",
    "        else:\n",
    "            # ann with payload\n",
    "            if ann_patterns[ann_name][\"has_payload\"]:\n",
    "                payload = get_payload(ann_name, ann_start_match, ann_end_match, text)\n",
    "            _, offset = get_base_idx(\n",
    "                ann_end_match, offset, is_end=True, payload_len=len(payload)\n",
    "            )\n",
    "            anns[ann_name].append((base_start, None, payload))\n",
    "\n",
    "        s_idx += 1\n",
    "        e_idx += 1\n",
    "\n",
    "    return anns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_base_vol_by_pages(vol_n):\n",
    "    vol_fn = config.peydurma_path/'inputs'/f'v{vol_n:03}-pg-annotated.txt'\n",
    "    vol_text = vol_fn.read_text()\n",
    "    # preprocess\n",
    "    vol_text = re.sub('<[^p].*?>', '', vol_text)\n",
    "    vol_text = re.sub('\\[\\d.*?\\]', '', vol_text)\n",
    "    vol_text = re.sub('\\[https.*?]\\n', '', vol_text)\n",
    "    # split by page\n",
    "    splits = re.split('<p.*?>', vol_text)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_pages = get_base_vol_by_pages(74)\n",
    "#len(test_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_base_text(text_id, opf_path, index_layer):\n",
    "    serializer = Serialize(opf_path, text_id=text_id, index_layer=index_layer)\n",
    "    #base_text = ''.join(serializer.get_text_base_layer().values())\n",
    "    base_text = list(serializer.get_text_base_layer().values())[0]\n",
    "    return base_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_text = get_base_text('D1109', config.tengyur_opf, index_layer)\n",
    "# base_text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract double-tsek from peyduma-tengyur text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.debug = False\n",
    "# peydurma_base_text, peydurma_ann_text = get_double_tsek_text(config.images_path/'I1PD95846')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(peydurma_ann_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Transfer annotation with patchs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def transfer_anns_with_patch(base_text, src_text, dest_text):\n",
    "    \"Apply patches computed from `src_text` to `dest_text` on `base_text\"\n",
    "    diffs = dmp.diff_main(src_text, dest_text)\n",
    "    patches = dmp.patch_make(src_text, diffs)\n",
    "    ann_text = dmp.patch_apply(patches, base_text)[0]\n",
    "    double_tsek_idxs = parse_double_tsek(ann_text)\n",
    "    print(f'\\t- Transferred {len(double_tsek_idxs)} out of {len(patches)}')\n",
    "    return double_tsek_idxs, ann_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Transfer annotations with diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def isNSM(char):\n",
    "    # Detects nonspacing mark characters\n",
    "    if unicodedata.category(char) == \"Mn\":\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'ཽ'\n",
    "isNSM(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_first_char_idx(text, char):\n",
    "    \"\"\"Return first char idx in `text` and -1 of not found\n",
    "\n",
    "    found har idx is expanded for whitespces after the char.\n",
    "    \"\"\"\n",
    "    is_char_found = False\n",
    "    idx = -1\n",
    "    for i in range(len(text)):\n",
    "        if not is_char_found and text[i] == char:\n",
    "            is_char_found = True\n",
    "            idx = i\n",
    "        elif is_char_found:\n",
    "            if text[i] != ' ': return i-1\n",
    "            elif i == len(text)-1: return i\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01|  ' '5'\n",
      "'01|  '\n"
     ]
    }
   ],
   "source": [
    "string = '01|  567 9'\n",
    "idx = get_first_char_idx(string, '|')\n",
    "print(repr(string[:idx+1]), repr(string[idx+1]))\n",
    "\n",
    "string = '01|  '\n",
    "idx = get_first_char_idx(string, '|')\n",
    "print(repr(string[:idx+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adjust_next_diff(i, diffs, ann_text, to_char):\n",
    "    diff_mode, diff_chunk = diffs[i+1]\n",
    "    # add chars till next tsek to the ann_text\n",
    "    first_char_idx = get_first_char_idx(diff_chunk, to_char)\n",
    "    ann_text += diff_chunk[:first_char_idx+1]\n",
    "    # remove chars till next tsek from diff[i+1]\n",
    "    diffs[i+1] = (diff_mode, diff_chunk[first_char_idx+1:])\n",
    "    return ann_text\n",
    "\n",
    "\n",
    "def transfer_anns_with_diff(inputs):\n",
    "    base_text, dest_text = inputs\n",
    "    ann_text = ''\n",
    "    diffs = dmp.diff_main(dest_text, base_text, checklines=False)\n",
    "    for i in range(len(diffs)):\n",
    "        if diffs[i][0] == -1:\n",
    "            if config.double_tsek_sym in diffs[i][1]:\n",
    "                # check for next diff adjustment\n",
    "                if i < len(diffs)-1:\n",
    "                    # adjust next diff[i+1] if it's first char is NSM\n",
    "                    if isNSM(diffs[i+1][1][0]):\n",
    "                        ann_text = adjust_next_diff(i, diffs, ann_text, config.tsek)\n",
    "\n",
    "                    # adjust next diff[i+1] if it's first char in tsek\n",
    "                    elif diffs[i+1][1][0] == config.tsek:\n",
    "                        ann_text = adjust_next_diff(i, diffs, ann_text, config.tsek)\n",
    "\n",
    "                    # adjust next diff[i+1] if it's first char in shed\n",
    "                    elif diffs[i+1][1][0] == config.shed:\n",
    "                        ann_text = adjust_next_diff(i, diffs, ann_text, config.shed)\n",
    "\n",
    "                    # adjust next diff[i+1] if its first char is line return\n",
    "                    elif diffs[i+1][1][0] == '\\n':\n",
    "                        ann_text = adjust_next_diff(i, diffs, ann_text, '\\n')\n",
    "\n",
    "                if len(diffs[i][1]) == 1:\n",
    "                    ann_text += diffs[i][1]\n",
    "                else:\n",
    "                    ann_text += '$'\n",
    "        else:\n",
    "            ann_text += diffs[i][1]\n",
    "    double_tsek_idxs = parse_double_tsek(ann_text)\n",
    "    print(f'\\t- Transferred {len(double_tsek_idxs)} out of {dest_text.count(config.double_tsek_sym)}')\n",
    "    return double_tsek_idxs, ann_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSM case\n",
    "# p_text = 'དེ་ནས་ཀོ$ཧཱུཾ་བཱི་ན་'\n",
    "# d_text = 'དེ་ནས་ཀཽ་ཤཱཾ་བཱི་ན་གནས་'\n",
    "# transfer_anns_with_diff(d_text, p_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust marker which is diffed before tsek\n",
    "# p_text = 'རིག་པས$༔མ་རིག་པའི་སྒོ་'\n",
    "# d_text = 'རིག་པས་མ་རིག་པའི་སྒོ་'\n",
    "# transfer_anns_with_diff(d_text, p_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_text = 'ནི༑$ལྷ་སྦྱིན་འདི་ཉིད་'\n",
    "# d_text = '་ནི། ལྷ་སྦྱིན་འདི་ཉིད་'\n",
    "# transfer_anns_with_diff(d_text, p_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_text = 'ཟད་དེ། $ད་ནི་དེ་'\n",
    "# d_text = 'ཟད་དེ།\\nད་ནི་དེ་'\n",
    "# transfer_anns_with_diff(d_text, p_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Parse the double-tsek from dergey-tengyur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pedurma_ann_text = 'ལས་བརྒྱ་ཐམ་པ་པ། ༄༅། །རྒྱ་གར་སྐད་དུ། ཀརྨ་ཤ་ཏ་ཀ། བོད་སྐད་དུ། ལས་བརྒྱ་ཐམ་པ་པོ། བམ་པོ་དང་པོ། ཐམས་ཅད་མཁྱེན་པ་ལ་ཕྱག་འཚལ་ལོ། །གང་ལས་འཇིག་རྟེན་བླ་མ་བདེ་གཤེགས་ཐོས་པའི་སྒོ་ནས་རབ་སྙན་བརྟན་པའི་གསུང་་་་ལྡན་གྱིས། །སེམས་ཅན་རྣམས་ལ་ཕན་པ་འབའ་ཞིག་བཞེད་ཕྱིར་བཤད་པ་རྣམ་པ་སྣ་ཚོགས་རང་ཉིད་ཀྱིས། །ལོག་པར་ལྟ་བའི་མུན་ནག་ཆེན་པོ་ཐིབས་པོར་$འཐོམས་ཤིངའཁྲུགས་པ་རྣམས་ལ་རབ་གསུངས་པ། །$དེ་ཡི་མིང་ནི་ལས་རྣམ་བརྒྱ་པ་ཞེས་བྱ་ཡོངས་སུ་ཚང་བ་བདག་གིས་བཤད་ཀྱིས་ཉོན། །སྤྱི་སྡོམ་ནི༑ཁྱི་མོ་དང་ནི་ཤིང་རྟ་དང་། །ཀ་ཙང་ཀ་ལ་བྱམས་མི་སྡུག། བྱ་དང་འཕྱེ་བོ་གང་པོ་དང་། །བུ་རྣམས་དང་ནི་བརྒྱ་བྱིན་ནོ། །སྡོམ་ནི།'\n",
    "# derge_text = '༄༅༅། །རྒྱ་གར་སྐད་དུ། ཀརྨ་ཤ་ཏ་ཀ། བོད་སྐད་དུ། ལས་བརྒྱ་ཐམ་པ་པ། བམ་པོ་དང་པོ། ཐམས་ཅད་མཁྱེན་པ་ལ་ཕྱག་འཚལ་ལོ། །གང་ལས་འཇིག་རྟེན་བླ་མ་བདེ་གཤེགས་ཐོས་པའི་སྒོ་ནས་རབ་སྙན་བརྟན་པའི་གསུང་ལྡན་གྱིས། །སེམས་ཅན་རྣམས་ལ་ཕན་པ་འབའ་ཞིག་བཞེད་ཕྱིར་བཤད་པ་རྣམ་པ་སྣ་ཚོགས་རང་ཉིད་ཀྱིས། །ལོག་པར་ལྟ་བའི་མུན་ནག་ཆེན་པོ་ཐིབས་པོར་འཐོམས་ཤིང་འཁྲུགས་པ་རྣམས་ལ་རབ་གསུངས་པ། །དེ་ཡི་མིང་ནི་ལས་རྣམ་བརྒྱ་པ་ཞེས་བྱ་ཡོངས་སུ་ཚང་བ་བདག་གིས་བཤད་ཀྱིས་ཉོན། །སྤྱི་སྡོམ་ནི། ཁྱི་མོ་དང་ནི་ཤིང་རྟ་དང་། །ཀ་ཙང་ཀ་ལ་བྱམས་མི་སྡུག །བྱ་དང་འཕྱེ་བོ་གང་པོ་དང་། །བུ་རྣམས་དང་ནི་བརྒྱ་བྱིན་ནོ། །སྡོམ་ནི། ཁྱི་མོ་མིག་ཆུང་'\n",
    "# transfer_anns_with_diff(derge_text, pedurma_ann_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-97-f065cfc4662a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-97-f065cfc4662a>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print(f'[INFO] Transferring double tsek for {text_id if text_id else f'v{vol_n:03}')\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "def run(p_vol_path, index_layer, text_id=None, vol_n=None, start=1, end=float('inf'), replace=False, engine='diff'):\n",
    "    print(f'[INFO] Transferring double tsek for {text_id if text_id else f'v{vol_n:03}')\n",
    "    out_idxs_fn = config.output_path/f'{text_id}-dtsek_idxs.txt'\n",
    "    ann_text_fn = config.output_path/f'{text_id}-ann_text.txt'\n",
    "    if not out_idxs_fn.is_file() or replace:\n",
    "        if text_id:\n",
    "            base_text = get_base_text(text_id, opf_path, index_layer)\n",
    "        else:\n",
    "            base_text = get_base_vol(vol_n, opf_path)\n",
    "            text_id = f'v{vol_n:03}'\n",
    "        p_base_text, p_ann_text = get_double_tsek_text(text_id, p_vol_path, start, end, engine)\n",
    "        if engine == 'diff':\n",
    "            double_tsek_idxs, ann_text = transfer_anns_with_diff(base_text, p_ann_text)\n",
    "        elif engine == 'patch':\n",
    "            double_tsek_idxs, ann_text = transfer_anns_with_patch(base_text, p_base_text, p_ann_text)\n",
    "\n",
    "        out_idxs_fn.write_text('\\n'.join(map(str, double_tsek_idxs)))\n",
    "        ann_text_fn.write_text(ann_text)\n",
    "    print(f'[INFO] Double tsek indices are save at {config.output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nbdev.sync import script2notebook\n",
    "script2notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#index_layer = get_index_layer(config.kangyur_opf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
